---
title: "Infer Phylogenetic Parameters with RNN from LTT"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

### Importing libraries 

```{r}
library(torch)
library(luz)
library(ggplot2)
library(MLmetrics)
library(svMisc)
source("summary-statistics.R")
source("neural-network-functions.R")
```


```{r}


n_trees <- 5200 # number of trees to generate 
n_taxa <- 100 # size of the trees
lambda_range <- c(0.15, 0.25) # range within random lambda will be generated 
mu_range <- c(0.0, 0.1) # same for mu

df.ltt <- data.frame("t1" = rep(NA,n_taxa)) # initialize data.frame
df.rates <- data.frame("lambda" = rep(NA, n_trees), "mu" = rep(NA, n_trees)) # targets 
lambda.vec <- c()
mu.vec <- c()
r.vec <- c()
epsilon.vec <- c()

for (i in 1:n_trees){
  progress(100*i/n_trees)
  lambda <- runif(1, lambda_range[1], lambda_range[2]) # generate a random spec. rate
  mu <- runif(1, mu_range[1], mu_range[2]) # same w/ ext. rate 
  r <- lambda - mu
  epsilon <- mu / lambda 
  lambda.vec <- c(lambda.vec, lambda)
  mu.vec <- c(mu.vec, mu)
  r.vec <- c(r.vec, r)
  epsilon.vec <- c(epsilon.vec, epsilon)
  tree <- trees(c(lambda, mu), "bd", max.taxa=n_taxa)[[1]]
  ltt.coord <- ape::ltt.plot.coords(tree)
  ltt.coord <- as.data.frame(ltt.coord)
  df.ltt[paste("t",i,sep="")] <- ltt.coord$time
}

df.rates$lambda <- lambda.vec
df.rates$mu <- mu.vec
df.rates$r <- r.vec
df.rates$epsilon <- epsilon.vec
```


```{r}
matrix.ltt <- df.ltt %>% as.matrix()
array.ltt  <- array(matrix.ltt, dim = c(n_taxa, n_trees, 1))
dim(array.ltt)
```

```{r}
ltt_dataset <- torch::dataset(
    
    name <- "ltt_dataset", 
    
    initialize = function(array.ltt, df.rates, direct_target){

      # input data 
      x <- array.ltt
      self$x <- x
      
      
      # target data 
      if (direct_target){target.names <- c("lambda", "mu")}
      else {target.names <- c("r", "epsilon")}
      y = df.rates[target.names] %>% 
        as.matrix()
      self$y <- torch_tensor(y)
  
    }, 
    
    .getitem = function(i) {
      list(x = self$x[ ,i, ], y = self$y[i, ])
    }, 
    
    .length = function() {
      self$y$size()[[1]]
    }
    
)
```



```{r}
# Parameters of the NN's training
n_train <- 5000
batch_size <- 64
n_epochs <- 100

# Creation of the test and train dataset
train_indices <- sample(1:n_trees, n_train) 
test_indices <- setdiff(1:n_trees, train_indices) 
array.ltt.train <- array(array.ltt[ ,train_indices,], dim = c(n_taxa, n_train,1))
array.ltt.test  <- array(array.ltt[ ,test_indices,], dim = c(n_taxa, n_trees - n_train,1))
train_ds <- ltt_dataset(array.ltt.train, df.rates[train_indices, ],
                        direct_target=TRUE)
test_ds  <- ltt_dataset(array.ltt.test, df.rates[test_indices, ], 
                        direct_target=TRUE)
  
# Creation of the dataloader 
train_dl <- train_ds %>% dataloader(batch_size=batch_size, shuffle=TRUE)
test_dl  <- test_ds  %>% dataloader(batch_size=batch_size, shuffle=FALSE)
```



```{r}
rnn.net <- nn_module(
  
  initialize = function() {
    self$rnn <- nn_rnn(input_size = n_taxa, hidden_size = 50, 
                       dropout = .01, num_layers = 3, nonlinearity = "relu")
    self$out <- nn_linear(50, 2)
  },
  
  forward = function(x) {
    x <- self$rnn(x)[[1]]
    x <- x[ , dim(x)[2], ]
    x %>% self$out() 
  }
  
)
```



```{r}
rnn.fit <- rnn.net %>%
    setup(
      loss = function(y_hat, y_true) nnf_mse_loss(y_hat, y_true),
      optimizer = optim_adam
    ) %>%
    fit(train_dl, epochs = n_epochs, valid_data = test_dl, 
        callbacks = list(luz_callback_early_stopping(patience = 3)))
```


```{r}
plot_loss_records(rnn.fit)
```


```{r}
preds <- predict(rnn.fit, test_dl)
preds.r <- as.matrix(preds)[,1]
preds.epsilon <- as.matrix(preds)[,2]
test_dl <- dataloader(test_ds, batch_size = n_trees - n_train)
targets <- (test_dl %>% dataloader_make_iter() %>% dataloader_next())$y %>% 
  as.matrix()
targets.r <- targets[,1]
targets.epsilon <- targets[,2]
```


```{r}

preds.list <- list(preds.r, preds.epsilon)
targets.list <- list(targets.r, targets.epsilon)

par(mfrow=c(1,2))

for (i in 1:2){
  pred <- preds.list[[i]]
  true <- targets.list[[i]]
  r2.lambda <- R2_Score(pred, true)
  r2.lambda <- format(round(r2.lambda, 3), nsmall = 3)
  plot(true, pred, main=paste(names[[i]], "- r2 =", r2.lambda, sep=" "))
  abline(0,1)
  fit = lm(pred ~ true)
  sig = summary(fit)$coefficients[2,4]
  abline(fit, col="red", lty = ifelse(sig < .05,1,2))
}
```


---
title: "Infer Phylogenetic Parameters with RNN from LTT"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

### Importing libraries 

```{r}
library(torch)
library(diversitree)
library(luz)
library(ggplot2)
library(MLmetrics)
library(svMisc)
source("summary-statistics.R")
source("neural-network-functions.R")
```


```{r}

n_trees <- 200 # number of trees to generate 
n_taxa <- 300 # size of the trees
lambda_range <- c(0.15, 0.25) # range within random lambda will be generated 
mu_range <- c(0.0, 0.1) # same for mu

df.ltt <- data.frame("t1" = rep(NA,n_taxa)) # initialize data.frame
df.rates <- data.frame("lambda" = rep(NA, n_trees), "mu" = rep(NA, n_trees)) # targets 
lambda.vec <- c()
mu.vec <- c()
r.vec <- c()
epsilon.vec <- c()

for (i in 1:n_trees){
  progress(100*i/n_trees)
  lambda <- runif(1, lambda_range[1], lambda_range[2]) # generate a random spec. rate
  mu <- runif(1, mu_range[1], mu_range[2]) # same w/ ext. rate 
  r <- lambda - mu
  epsilon <- mu / lambda 
  lambda.vec <- c(lambda.vec, lambda)
  mu.vec <- c(mu.vec, mu)
  r.vec <- c(r.vec, r)
  epsilon.vec <- c(epsilon.vec, epsilon)
  tree <- trees(c(lambda, mu), "bd", max.taxa=n_taxa)[[1]]
  ltt.coord <- ape::ltt.plot.coords(tree)
  ltt.coord <- as.data.frame(ltt.coord)
  df.ltt[paste("t",i,sep="")] <- ltt.coord$time
}

df.rates$lambda <- lambda.vec
df.rates$mu <- mu.vec
df.rates$r <- r.vec
df.rates$epsilon <- epsilon.vec
```


```{r}
generate_ltt_dataframe <- function(n_trees, n_taxa, lambda_range, mu_range){
  
  dataframes <- list("ltt" = NA, "rates" = NA)
  
  df.ltt <- data.frame("t1" = rep(NA,n_taxa)) # initialize data.frame
  df.rates <- data.frame("lambda" = rep(NA, n_trees), "mu" = rep(NA, n_trees)) # targets 
  lambda.vec <- c()
  mu.vec <- c()
  r.vec <- c()
  epsilon.vec <- c()
  
  for (i in 1:n_trees){
    progress(100*i/n_trees)
    lambda <- runif(1, lambda_range[1], lambda_range[2]) # generate a random spec. rate
    mu <- runif(1, mu_range[1], mu_range[2]) # same w/ ext. rate 
    r <- lambda - mu
    epsilon <- mu / lambda 
    lambda.vec <- c(lambda.vec, lambda)
    mu.vec <- c(mu.vec, mu)
    r.vec <- c(r.vec, r)
    epsilon.vec <- c(epsilon.vec, epsilon)
    tree <- trees(c(lambda, mu), "bd", max.taxa=n_taxa)[[1]]
    ltt.coord <- ape::ltt.plot.coords(tree)
    ltt.coord <- as.data.frame(ltt.coord)
    df.ltt[paste("t",i,sep="")] <- ltt.coord$time
  }
  
  df.rates$lambda <- lambda.vec
  df.rates$mu <- mu.vec
  df.rates$r <- r.vec
  df.rates$epsilon <- epsilon.vec
  
  dataframes$ltt <- df.ltt
  datafra
}
```



```{r}
matrix.ltt <- df.ltt %>% as.matrix()
array.ltt  <- array(matrix.ltt, dim = c(n_taxa, n_trees, 1))
dim(array.ltt)
```


```{r}
ltt_dataset <- torch::dataset(
    
    name <- "ltt_dataset", 
    
    initialize = function(array.ltt, df.rates, direct_target){

      # input data 
      x <- array.ltt
      self$x <- x
      
      
      # target data 
      if (direct_target){target.names <- c("lambda", "mu")}
      else {target.names <- c("r", "epsilon")}
      y = df.rates[target.names] %>% 
        as.matrix()
      self$y <- torch_tensor(y)
  
    }, 
    
    .getitem = function(i) {
      list(x = self$x[i, , ,drop=TRUE], y = self$y[i, ])
    }, 
    
    .length = function() {
      self$y$size()[[1]]
    }
    
)
```


```{r}
# Parameters of the NN's training
n_train <- 5000
batch_size <- 64
n_epochs <- 100

# Creation of the test and train dataset
train_indices <- sample(1:n_trees, n_train) 
test_indices <- setdiff(1:n_trees, train_indices) 
array.ltt.train <- array(array.ltt[ ,train_indices,], dim = c(n_taxa, n_train,1))
array.ltt.test  <- array(array.ltt[ ,test_indices,], dim = c(n_taxa, n_trees - n_train,1))

array.ltt.train <- array(t(array.ltt[ ,train_indices,]), dim = c(n_train, n_taxa,1))
array.ltt.test  <- array(t(array.ltt[ ,test_indices,]), dim = c( n_trees - n_train, n_taxa,1))
train_ds <- ltt_dataset(array.ltt.train, df.rates[train_indices, ],
                        direct_target=TRUE)
test_ds  <- ltt_dataset(array.ltt.test, df.rates[test_indices, ], 
                        direct_target=TRUE)
  
# Creation of the dataloader 
train_dl <- train_ds %>% dataloader(batch_size=batch_size, shuffle=TRUE)
valid_dl  <- test_ds  %>% dataloader(batch_size=batch_size, shuffle=FALSE)
test_dl  <- test_ds  %>% dataloader(batch_size=1, shuffle=FALSE)
```


```{r}
rnn.net <- nn_module(
  
  initialize = function(n_input, n_hidden, n_layer, p_dropout = .01,
                        batch_first = TRUE) {
    self$rnn <- nn_lstm(input_size = n_input, hidden_size = n_hidden, 
                       dropout = p_dropout, num_layers = n_layer,
                       batch_first = batch_first)
    self$out <- nn_linear(n_hidden, 2)
  },
  
  forward = function(x) {
    x <- self$rnn(x)[[1]]
    x <- x[, dim(x)[2], ]
    x %>% self$out() 
  }
  
)
```


```{r}
rnn.fit <- rnn.net %>%
    setup(
      loss = function(y_hat, y_true) nnf_mse_loss(y_hat, y_true),
      optimizer = optim_adam
    ) %>%
    fit(train_dl, epochs = n_epochs, valid_data = test_dl, 
        callbacks = list(luz_callback_early_stopping(patience = 3)))
```


```{r}

net <- rnn.net(1, 50, 4, 0.)
net$to(device = device)

n_epochs <- 15

opt <- optim_adam(params = net$parameters)

train_batch <- function(b){
  opt$zero_grad()
  output <- net(b$x$reshape(c(b$x$shape, 1L))$to(device = device))
  target <- b$y$to(device = device)
  
  loss <- nnf_mse_loss(output, target)
  loss$backward()
  opt$step()
  
  loss$item()
}


valid_batch <- function(b) {
  
  output <- net(b$x$reshape(c(b$x$shape, 1L))$to(device = device))
  target <- b$y$to(device = device)
  
  loss <- nnf_mse_loss(output, target)
  
  loss$item()
  
}


for (epoch in 1:n_epochs) {
  
  net$train()
  train_loss <- c()
  
  coro::loop(for (b in train_dl) {
    loss <-train_batch(b)
    train_loss <- c(train_loss, loss)
  })
  
  cat(sprintf("epoch %0.3d/%0.3d - train - loss: %3.5f \n", epoch, n_epochs, mean(train_loss)))
  
  net$eval()
  valid_loss <- c()

  coro::loop(for (b in test_dl) {
    loss <- valid_batch(b)
    valid_loss <- c(valid_loss, loss)
  })
  
  cat(sprintf("epoch %0.3d/%0.3d - valid - loss: %3.5f \n", epoch, n_epochs, mean(valid_loss)))
}
```

```{r}
net$eval()

lambda_preds <- c()
mu_preds <- c()
lambda_true <- c()
mu_true <- c()

coro::loop(for (b in test_dl) {
  
  output <- net(b$x$reshape(c(b$x$shape, 1L))$to(device = device))
  preds <- as.numeric(output$to(device = "cpu"))
  true <- as.numeric(b$y)

  lambda_preds <- c(lambda_preds, preds[1])
  mu_preds <- c(mu_preds, preds[2])
  lambda_true <- c(lambda_true, true[1])
  mu_true <- c(mu_true, true[2])
  
})

```


```{r}
par(mfrow=c(1,2))

plot(lambda_true, lambda_preds)
abline(0,1)
plot(mu_true, mu_preds)
abline(0,1)
```


```{r}
net$train()
batch <- dataloader_make_iter(train_dl) %>% dataloader_next()
X = batch[[1]]$reshape(list(64L, 50L, 1L))
Y = batch[[2]]
opt <- optim_adam(params = model$parameters)
opt$zero_grad()
p = net(X$to(device = ("cuda:0")))
loss = nnf_mse_loss(p, Y$to(device = "cuda:0"))
loss$backward()
opt$step()
```


```{r}
plot_loss_records(rnn.fit)
```


```{r}
preds <- predict(rnn.fit, test_dl)
preds.r <- as.matrix(preds)[,1]
preds.epsilon <- as.matrix(preds)[,2]
test_dl <- dataloader(test_ds, batch_size = n_trees - n_train)
targets <- (test_dl %>% dataloader_make_iter() %>% dataloader_next())$y %>% 
  as.matrix()
targets.r <- targets[,1]
targets.epsilon <- targets[,2]
```


```{r}

preds.list <- list(preds.r, preds.epsilon)
targets.list <- list(targets.r, targets.epsilon)

par(mfrow=c(1,2))

for (i in 1:2){
  pred <- preds.list[[i]]
  true <- targets.list[[i]]
  r2.lambda <- R2_Score(pred, true)
  r2.lambda <- format(round(r2.lambda, 3), nsmall = 3)
  plot(true, pred, main=paste(names[[i]], "- r2 =", r2.lambda, sep=" "))
  abline(0,1)
  fit = lm(pred ~ true)
  sig = summary(fit)$coefficients[2,4]
  abline(fit, col="red", lty = ifelse(sig < .05,1,2))
}
```


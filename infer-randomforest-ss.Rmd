---
title: "Infer Phylogenetic Parameters with RandomForest from Summary Statistics"
output: html_notebook
---


### Importing librairies & sources 

```{r echo=T}
library(randomForest)
library(MLmetrics)
source("summary-statistics.R")
```


The idea is to infer the speciation and extinction rates (resp. $\lambda$ and 
$\mu$) of a phylogenetic tree from its summary statistics (see Saulnier 2017) 
using a random forest. 
$\lambda$ and $\mu$ can be inferred directly, or we can infer another related 
couple of variables: $r = \lambda - \mu$ and $\epsilon = \frac{\mu}{\lambda}$.
We will see which couple leads to the best prediction. 

To begin we generate a large number of trees in order to train the random forest.

```{r echo = T, results='hide'}
# Parameters 
n_trees_train <- 1000 # number of trees for the training 
n_taxa <- 100 # size of the tree 
lambda_range <- c(0.1, 0.5) # range within random lambda will be generated 
mu_range <- c(0.02, 0.06) # same for mu

# Generate the train set 
df_train <- generate_ss_dataframe(n_trees_train, n_taxa,
                                  lambda_range[1], lambda_range[2],
                                  mu_range[1], mu_range[2])
```

We generate also a test set, to test the predictions of the random forest. 

```{r echo = T, results='hide'}
n_trees_test <- 200 # number of trees for test
# Other parameters remains the same as above 

df_test <- generate_ss_dataframe(n_trees_test, n_taxa,
                                  lambda_range[1], lambda_range[2],
                                  mu_range[1], mu_range[2])
```

Let's see how our training data.frame looks like. It has: 

- 500 rows, one row per tree generated;
- 82 columns, 78 columns for summary statistics + 4 columns for $\lambda,\mu,r,\epsilon$.

```{r echo=T}
df_train
```

Train one random forest per parameter.

```{r}

# Couple 1 - Lambda 
model.lambda <- randomForest(
  formula = lambda ~ .- mu - epsilon - r,
  ntree = 500,
  data = df_train
)
model.lambda

# Couple 1 - Mu
model.mu <- randomForest(
  formula = mu ~ .- lambda - epsilon - r,
  ntree = 500,
  data = df_train
)
model.mu

# Couple 2 - R
model.r <- randomForest(
  formula = r ~ .- epsilon - lambda - mu,
  ntree = 500,
  data = df_train
)
model.r

# Couple 2 - Epsilon
model.epsilon <- randomForest(
  formula = epsilon ~ .- r - lambda - mu,
  ntree = 500,
  data = df_train
)
model.epsilon
```
Make prediction on the test set for each random forest (*i.e.* each parameter). 

```{r}
pred.lambda <- predict(model.lambda, newdata=df_test)
pred.mu <- predict(model.mu, newdata=df_test)
pred.r <- predict(model.r, newdata=df_test)
pred.epsilon <- predict(model.epsilon, newdata=df_test)
```

```{r echo=F}

preds <- list(pred.lambda, pred.mu, pred.r, pred.epsilon)
valid <- list(df_test$lambda, df_test$mu, df_test$r, df_test$epsilon)
names <- list("lambda -", "mu -", "r -", "epsilon -")

par(mfrow=c(2,2))

for (i in 1:4){
  pred <- preds[[i]]
  true <- valid[[i]]
  r2.lambda <- R2_Score(pred, true)
  r2.lambda <- format(round(r2.lambda, 3), nsmall = 3)
  plot(true, pred, main=paste(names[[i]], "r2 =", r2.lambda, sep=" "))
  abline(0,1)
}
```

Quantify the error (MSE)

```{r}
sqrt(mean((pred.lambda - df_test$lambda)^2)/mean(df_test$lambda))
sqrt(mean((pred.mu - df_test$mu)^2)/mean(df_test$mu))
sqrt(mean((pred.epsilon - df_test$epsilon)^2))
R2_Score(pred.epsilon,df_test$epsilon)
```


In each random forest the importance of the different predictors (summary 
statistics here) can be investigated as follow: 

```{r}
model.lambda$importance
```
